{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets        import make_classification\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"01.df.procesado.csv\")\n",
    "# df_original = pd.read_csv(\"01.df.procesado.csv\")\n",
    "df_original = pd.read_csv('https://raw.githubusercontent.com/blukitas/AA-2021/main/TPs/02.TP/01.df.procesado.csv')\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = [col for col in df_original.columns if df_original[col].dtype != \"object\"]\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "    \"file_path\",\n",
    "    \"Unnamed: 0\",\n",
    "    \"modality\",\n",
    "    \"vocal_channel\",\n",
    "#     \"emotion\",\n",
    "    \"emotional_intensity\",\n",
    "    \"statement\",\n",
    "    \"repetition\",\n",
    "    \"actor\",\n",
    "]\n",
    "num_columns = [x for x in num_columns if x not in drop_columns]\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df_original[num_columns] #.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=random_state)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_proc.loc[:, df_proc.columns != \"emotion\"],\n",
    "    df_proc[\"emotion\"],\n",
    "    stratify=df_proc[\"emotion\"],\n",
    "    random_state=66,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adrian notebook\n",
    "\n",
    "Ejemplo que paso adrián mostranod un poco las funciones de activacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcs activacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalon\n",
    "binary_step = lambda z: 1.0 if z >= 0 else 0.0\n",
    "\n",
    "parametric_relu = lambda alpha: lambda z: np.maximum(0, z) * alpha\n",
    "\n",
    "relu = parametric_relu(alpha=1)\n",
    "\n",
    "leaky_relu = parametric_relu(alpha=0.01)\n",
    "\n",
    "parametric_elu = lambda alpha: lambda z:  alpha * (np.exp(z) -1) if z < 0 else z\n",
    "\n",
    "elu = parametric_elu(alpha=1.0)\n",
    "\n",
    "sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "\n",
    "identity = lambda z: z\n",
    "\n",
    "tanh = lambda z: (2 / (1 + np.exp(-2 * z))) -1\n",
    "\n",
    "arc_tan = lambda z: np.arctan(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrica optimizar -> MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda y_true, x_true: sum((y_true - x_true) ** 2) / len(x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, act_fn, inputs_count):\n",
    "        self.act_fn = act_fn\n",
    "        # Initilizamos los pesos con valores random...\n",
    "        self.weights = [random.random() for _ in range(0, inputs_count)]\n",
    "\n",
    "    def predict(self, features):\n",
    "        output = sum([w * f for w, f in zip(self.weights, features)])\n",
    "        return self.act_fn(output)\n",
    "\n",
    "    def fit(\n",
    "        self, \n",
    "        X_train, y_train, X_val, y_val, \n",
    "        metric_fn, \n",
    "        learning_rate, \n",
    "        epocs,\n",
    "        verbose=False\n",
    "    ):\n",
    "        summary = []\n",
    "\n",
    "        # Repetimos el proceso de entenamiento \"epocs\" veces...\n",
    "        for epoc in range(1, epocs -1):\n",
    "            \n",
    "            # Ajustamos los pesos del modelo...\n",
    "            for features, y_true in zip(X_train, y_train):\n",
    "                y_pred = self.predict(features)\n",
    "                self.__back_propagation(features, y_pred, y_true, learning_rate)\n",
    "\n",
    "            # Calculamos la metrica para el conjunto de validacion y train\n",
    "            metric_values = self.__calculate_metric(\n",
    "                metric_fn, \n",
    "                X_train, y_train,  \n",
    "                X_val, y_val\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f'epoc: {epoc}, train: {metric_values[0]}, val: {metric_values[1]}')\n",
    "            \n",
    "            summary.append(metric_values)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def __back_propagation(self, features, y_pred, y_true, learning_rate):\n",
    "        # Ajustamos los pesos del modelo, dado un ejemplo a aprender...\n",
    "        for index, feature in enumerate(features):\n",
    "            self.weights[index] += learning_rate * (y_true - y_pred) * feature\n",
    "\n",
    "    def __calculate_metric(self, metric_fn, X_train, y_train, X_val, y_val):\n",
    "        train_metric_value = metric_fn([self.predict(x) for x in X_train], y_train)                                                                                    \n",
    "        val_metric_value   = metric_fn([self.predict(x) for x in X_val], y_val)\n",
    "        return train_metric_value, val_metric_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Funcion para graficar train vs validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(summary):\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.lineplot(data=[summary[i][0] for i in range(0, len(summary))], label='Train')\n",
    "    sns.lineplot(data=[summary[i][1] for i in range(0, len(summary))], label='Validation')\n",
    "    plt.xlabel(\"Epocs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"MSE: Train vs. Validation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objeto que permite realizar corridas del modelo con distintos hiper-parametros y ademas grafica las curvas de evolucion de la metrica elegida (MSE) en train y evaluation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTest:\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val   = X_val\n",
    "        self.y_val   = y_val\n",
    "        # La cantidad de entradas es igual a la cantidad de features...\n",
    "        self.inputs_count = len(X_train[0])\n",
    "\n",
    "    def perform(\n",
    "        self,\n",
    "        # Function de activacion\n",
    "        act_fn, \n",
    "        metric_fn     = mse,\n",
    "        learning_rate = 0.00001, \n",
    "        epocs         = 350,\n",
    "        verbose       = False\n",
    "    ):\n",
    "        model = Perceptron(act_fn, self.inputs_count)\n",
    "\n",
    "        summary = model.fit(\n",
    "            self.X_train,\n",
    "            self.y_train,\n",
    "            self.X_val, \n",
    "            self.y_val,\n",
    "            metric_fn,\n",
    "            learning_rate, \n",
    "            epocs,\n",
    "            verbose\n",
    "        )\n",
    "\n",
    "        plot_metrics(summary)\n",
    "\n",
    "        print('Val MSE:', summary[-1][1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ModelTest(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación escalon:\n",
    "test.perform(act_fn = binary_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación Sigmoide:\n",
    "test.perform(act_fn = sigmoid, epocs = 3_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación ReLU:\n",
    "test.perform(act_fn = relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación Leaky-ReLU:\n",
    "test.perform(act_fn = leaky_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación Parametric-ReLU:\n",
    "test.perform(act_fn = parametric_relu(alpha = 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación Identidad:\n",
    "test.perform(act_fn = identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación tangente hiperbólica:\n",
    "test.perform(act_fn = tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación ELU:\n",
    "test.perform(act_fn = elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación Parametric-ELU:\n",
    "test.perform(act_fn = parametric_elu(alpha = 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aprendizaje usando la funcion de activación arcotangente:\n",
    "test.perform(act_fn = arc_tan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA - Siguiendo clase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimensiones: {}'.format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_in = tfkl.Input(shape=(28,28)) #Todo modelo necesita una entrada, y debemos especificar sus dimensiones\n",
    "#Esta capa rompe la estructura de imagen y nos deja un vector (1x784)\n",
    "flatten_layer = tfkl.Flatten()(layer_in)\n",
    "#Esta es la capa de salida\n",
    "hidden_layer = tfkl.Dense(units=128, activation='relu')(flatten_layer)\n",
    "#Esta es la capa de salida:\n",
    "output_layer = tfkl.Dense(units=10,activation='softmax')(hidden_layer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = tf.keras.Model(inputs=[layer_in],outputs=[output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(mlp_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "metrics_list = [tf.keras.metrics.CategoricalAccuracy(),\n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.AUC()]\n",
    "\n",
    "mlp_model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=metrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, hay que pasarle al modelo los datos de entrenamiento.\n",
    "\n",
    "Primero abrimos tensorboard, el cual nos permite monitorear la performance del modelo durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'tblogs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible especificar funciones o callbacks que seran llamadas durante el entrenamiento del modelo. En este caso usamos:\n",
    "\n",
    "* **TensorBoard**: guarda métricas y logs del entrenamiento para su visualización en TensorBoard.\n",
    "* **ModelCheckpoint**: guarda los parámetros del modelo al finalizar cada época de entrenamiento, lo cual es útil para restaurar modelos o retomar el entrenamiento si se interrumpe.\n",
    "* **EarlyStopping**: es una estrategia para evitar el sobre-entrenamiento, la cual consiste en dejar de entrenar el modelo si el error de validación deja de disminuir. En este caso, monitor es la métrica que vamos a analizar para decidir cuando deja de mejorar el modelo, y patience indica cuántas épocas sin mejorar el mejor modelo hay que esperar antes de detener el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_list = [tf.keras.callbacks.TensorBoard(log_dir='tblogs'),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath='checkpoints'),\n",
    "          tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)]\n",
    "\n",
    "mlp_model.fit(x=train_images,y=train_labels,validation_data=(val_images,val_labels),batch_size=128,epochs=50,callbacks=cb_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos buscar el tamaño óptimo de la capa oculta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "hyperparameter_space = {'units': np.arange(10,1000,20),'activation': ['relu','tanh','sigmoid','elu'],'batch_size':[16,32,64,128,256,512]}\n",
    "hyperparameters = list(ParameterSampler(hyperparameter_space,n_iter=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para emprolijar las cosas puedo ahora armar una función con todo lo que hecho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo(hparams):\n",
    "    layer_in = tfkl.Input(\n",
    "        shape=(28, 28)\n",
    "    )  # Todo modelo necesita una entrada, y debemos especificar sus dimensiones\n",
    "    # Esta capa rompe la estructura de imagen y nos deja un vector (1x784)\n",
    "    flatten_layer = tfkl.Flatten()(layer_in)\n",
    "    # Esta es la capa de salida\n",
    "    hidden_layer = tfkl.Dense(units=hparams[\"units\"], activation=hparams[\"activation\"])(\n",
    "        flatten_layer\n",
    "    )\n",
    "    # Esta es la capa de salida:\n",
    "    output_layer = tfkl.Dense(units=10, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "    mlp_model = tf.keras.Model(inputs=[layer_in], outputs=[output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    metrics_list = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "    ]\n",
    "    metric_names = [\"loss\", \"acc\", \"precision\", \"recall\", \"auc\"]\n",
    "    mlp_model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=metrics_list\n",
    "    )\n",
    "\n",
    "    cb_list = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=\"tblogs\"),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=\"checkpoints\"),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "    ]\n",
    "\n",
    "    mlp_model.fit(\n",
    "        x=train_images,\n",
    "        y=train_labels,\n",
    "        validation_data=(val_images, val_labels),\n",
    "        batch_size=hparams[\"batch_size\"],\n",
    "        epochs=50,\n",
    "        callbacks=cb_list,\n",
    "    )\n",
    "    performance = mlp_model.evaluate(x=val_images, y=val_labels)\n",
    "    performance = dict(zip(metric_names, performance))\n",
    "\n",
    "    return mlp_model, performance, hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recall = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "hyp_results = []\n",
    "for hparam in hyperparameters:\n",
    "    model, perf, hparams = entrenar_modelo(hparam)\n",
    "    if perf[\"recall\"] > best_recall:\n",
    "        best_recall = perf[\"recall\"]\n",
    "        best_model = model\n",
    "        best_params = hparams\n",
    "    hyp_results.append({\"params\": hparams, \"performance\": perf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficar parametros - performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"activation\"\n",
    "y = \"auc\"\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(15, 10))\n",
    "x_labels = [\"activation\", \"batch_size\", \"units\"]\n",
    "y_labels = [\"acc\", \"auc\", \"loss\", \"precision\", \"recall\"]\n",
    "for i, y in enumerate(y_labels):\n",
    "    for j, x in enumerate(x_labels):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for run in hyp_results:\n",
    "            xs.append(run[\"params\"][x])\n",
    "            ys.append(run[\"performance\"][y])\n",
    "        if i == len(y_labels) - 1:\n",
    "            ax[i, j].set_xlabel(x)\n",
    "        if j == 0:\n",
    "            ax[i, j].set_ylabel(y)\n",
    "        ax[i, j].scatter(xs, ys, alpha=0.6, c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance = best_model.evaluate(x=test_images,y=test_labels)\n",
    "metric_names = ['loss','acc','precision','recall','auc']\n",
    "test_performance = dict(zip(metric_names,test_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
